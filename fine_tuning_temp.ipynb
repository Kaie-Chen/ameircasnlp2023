{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "from typing import Dict, List, Optional\n",
    "from collections import Counter\n",
    "import csv\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torchmetrics\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(src_filepath: str, target_filepath: str = None):\n",
    "    data = {'src_text': []}\n",
    "    with open(src_filepath) as f:\n",
    "        for line in f:\n",
    "            data['src_text'].append(line.strip())\n",
    "\n",
    "    if target_filepath:\n",
    "        data['target_text'] = []\n",
    "        with open(target_filepath) as f:\n",
    "            for line in f:\n",
    "                data['target_text'].append(line.strip())\n",
    "\n",
    "    return data\n",
    "\n",
    "def predict(model: nn.Module, dataloader: DataLoader, tokenizer: AutoTokenizer, device: torch.device) -> List[List[str]]:\n",
    "    ### Temperaty \n",
    "\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\").to(device)\n",
    "            logits = model.generate(**inputs, max_length=512,\n",
    "                                    forced_bos_token_id=tokenizer.lang_code_to_id[\"spa_Latn\"])\n",
    "\n",
    "            # only consider non-padded tokens\n",
    "            # impement later\n",
    "            \n",
    "            preds.append(tokenizer.batch_decode(logits, skip_special_tokens=True))\n",
    "                    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDataset:   \n",
    "    ### Temperay dataset class until issue 1 fixed \n",
    "\n",
    "    def __init__(self, raw_data: Dict[str, List[str]], src_tokenizer: AutoTokenizer, target_tokenizer: AutoTokenizer = None):\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.src_text = raw_data['src_text']\n",
    "        self.target_text = []\n",
    "        self.with_target = False\n",
    "\n",
    "        if 'target_text' in raw_data:\n",
    "            if not target_tokenizer:\n",
    "                raise Exception(\"Expect tokenizer for target language: Got None\")\n",
    "\n",
    "            self.with_target = True\n",
    "            self.target_tokenizer = target_tokenizer\n",
    "            self.target_text = raw_data['target_text']\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src_text)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.with_target:\n",
    "            # for training and validation\n",
    "            return self.src_text[idx], self.target_text[idx]\n",
    "        else:\n",
    "            # for testing\n",
    "            return self.src_text[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loading . . . . . . . . . . . . . . . .\n",
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "main_folder =  './processed_data/'\n",
    "aymara_folder = main_folder + 'aymara/'\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "print(\"Model Loading . . . . . . . . . . . . . . . .\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aymara_dev_raw = load_raw_data(aymara_folder + 'dev.aym')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=\"aym_Latn\", padding='max_length', truncation=256, max_length=256)\n",
    "\n",
    "aymara_dev_data = LanguageDataset(aymara_dev_raw, tokenizer)\n",
    "aymara_dev_dataloader = DataLoader(aymara_dev_data, batch_size = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [256018, 32360, 80651, 21616, 2], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ISSUE 1 \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=\"ayr_Latn\", padding='max_length', max_length=20)\n",
    "\"\"\"\n",
    "https://huggingface.co/docs/transformers/model_doc/nllb\n",
    "padding (bool, str or PaddingStrategy, optional, defaults to False) â€” Activates and controls padding. Accepts the following values:\n",
    "True or 'longest': Pad to the longest sequence in the batch (or no padding if only a single sequence if provided).\n",
    "'max_length': Pad to a maximum length specified with the argument max_length or to the maximum acceptable input length for the model if that argument is not provided.\n",
    "False or 'do_not_pad' (default): No padding (i.e., can output a batch with sequences of different lengths).\n",
    "\n",
    "However, the return still have no padding. Therefore, cannot generate tensor for batch_size > 1. For now batch_size are set to 1.\n",
    "\"\"\"\n",
    "tokenizer(aymara_dev_raw['src_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(model, aymara_dev_dataloader, tokenizer, device)\n",
    "with open(aymara_folder+\"pretrain_result.txt\", \"w\") as f:\n",
    "    for text in preds:\n",
    "        f.write(\" \".join(text) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
