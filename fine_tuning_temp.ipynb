{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "from typing import Dict, List, Optional\n",
    "from collections import Counter\n",
    "import csv\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torchmetrics\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "from LanguageDataset import LanguageDataset\n",
    "from utils import load_raw_data, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loading . . . . . . . . . . . . . . . .\n",
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "main_folder =  './processed_data/'\n",
    "aymara_folder = main_folder + 'aymara/'\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "print(\"Model Loading . . . . . . . . . . . . . . . .\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module, \n",
    "    dataloader: DataLoader,\n",
    "    tokenizer: AutoTokenizer, \n",
    "    optimizer: optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "):\n",
    "    # acc_metric NOT implimented\n",
    "    loss_metric = torchmetrics.MeanMetric(compute_on_step=False).to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # loop through all batches in the training\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids, input_mask, tags = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        input_ids = input_ids.t().view(seq_len, batch_size)\n",
    "        tags = tags.t().view(seq_len, batch_size)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model.generate(**inputs, max_length=256,\n",
    "                                forced_bos_token_id=tokenizer.lang_code_to_id[\"spa_Latn\"])\n",
    "\n",
    "        loss = cross_entropy_loss(logits, tags)\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_metric.update(loss, input_mask.numel() - input_mask.sum())\n",
    "        is_active = torch.logical_not(input_mask.t().view(seq_len, batch_size))  # non-padding elements\n",
    "        # only consider non-padded tokens when calculating accuracy\n",
    "    \n",
    "    print(f\"| Epoch {epoch} | loss {loss_metric.compute():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aymara_dev_raw = load_raw_data(aymara_folder + 'dev.aym')\n",
    "aymara_dev_raw['src_text'] = aymara_dev_raw['src_text'][:50]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=\"ayr_Latn\")\n",
    "\n",
    "aymara_dev_data = LanguageDataset(aymara_dev_raw)\n",
    "aymara_dev_dataloader = DataLoader(aymara_dev_data, batch_size = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:15<00:00,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = predict(model, aymara_dev_dataloader, AutoTokenizer.from_pretrained(model_name, src_lang=\"ayr_Latn\"), device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(aymara_dev_raw['src_text'][:2], padding='max_length', truncation=True, max_length=256, return_tensors=\"pt\").to(device)\n",
    "logits = model.generate(**inputs, max_length=256, forced_bos_token_id=tokenizer.lang_code_to_id[\"spa_Latn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2, 256161,  66948,   1125,  31915,      2,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1],\n",
       "        [     2, 256161,   1446,   2048,  26192, 237854,    356,    629,   5766,\n",
       "            153,    336,  51015,     79,  18442,  80826,  18550, 248075,      2]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(aymara_folder+\"pretrain_result.txt\", \"w\") as f:\n",
    "    for token_ids in preds:\n",
    "        text = tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n",
    "        f.write(\" \".join(text) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
