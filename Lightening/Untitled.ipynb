{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "983ed9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e997f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LanguageDataset\n",
    "\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "class LanguageDataset(Dataset):   \n",
    "    def __init__(self, raw_data):\n",
    "        self.data = raw_data\n",
    "        #self.s = spm.SentencePieceProcessor(model_file = pathToSPM + 'flores200_sacrebleu_tokenizer_spm.model')\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    \n",
    "    def processData(src_text, trg_text, src_code = 0, max_length = 64):\n",
    "        input_id = list(map(lambda x : x + 1, self.s.encode(src_text)[0]))\n",
    "        input_id.append(2)\n",
    "        if src_code != 0:\n",
    "            input_id.append(src_code)\n",
    "        padding = max_length - len(input_id)\n",
    "        if padding > 0:\n",
    "            for _ in range(padding):\n",
    "                input_id.append(1)\n",
    "        elif padding < 0:\n",
    "            input_id = input_id[:62]\n",
    "            input_id.append(2)\n",
    "            if src_code != 0:\n",
    "                input_id.append(src_code)\n",
    "            else:\n",
    "                input_id.append(1)\n",
    "        input_id = torch.tensor(input_id)\n",
    "        \n",
    "        attention_mask = torch.tensor([[1] * len(input_id)])\n",
    "\n",
    "        if trg_text is not None:\n",
    "            labels = list(map(lambda x : x + 1, self.s.encode(trg_text)[0]))\n",
    "            labels.append(2)\n",
    "            labels.append(256161)\n",
    "            padding = max_length - len(labels)\n",
    "            if padding > 0:\n",
    "                for _ in range(padding):\n",
    "                    labels.append(1)\n",
    "            elif padding < 0:\n",
    "                labels = labels[:62]\n",
    "                labels.append(2)\n",
    "                labels.append(256161)\n",
    "            labels = torch.tensor(labels)\n",
    "\n",
    "    \n",
    "    def load_raw_data(src_filepath: List[str], lang_code: List[str], model_name: str, \n",
    "                      trg_filepath: List[str]=None, max_length: int=256):\n",
    "        len_src = len(src_filepath)\n",
    "        text_data = {'src_text': []}\n",
    "        token_data = []\n",
    "        if len_src != len(lang_code):\n",
    "            raise Exception(\"Lengths of src_filepath and lang_code don't match.\")\n",
    "\n",
    "        if trg_filepath:\n",
    "            text_data['target_text'] = []\n",
    "            if len_src != len(trg_filepath):\n",
    "                raise Exception(\"Lengths of src_filepath and trg_filepath don't match.\")\n",
    "            is_trg = True\n",
    "\n",
    "        for i in range(len_src):\n",
    "            src_path = src_filepath[i]\n",
    "            code = lang_code[i]\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=code, tgt_lang=\"spa_Latn\")\n",
    "            with open(src_path) as f:\n",
    "                for line in f:\n",
    "                    text_data['src_text'].append(line.strip())\n",
    "\n",
    "            if is_trg:\n",
    "                trg_path = trg_filepath[i] \n",
    "                with open(trg_path) as f:\n",
    "                    for line in f:\n",
    "                        text_data['target_text'].append(line.strip())\n",
    "\n",
    "                for src_text, trg_text in zip(text_data['src_text'], text_data['target_text']):\n",
    "                    token_data.append(tokenizer(src_text, text_target=trg_text, \n",
    "                                                max_length=max_length, padding='max_length', truncation=True))\n",
    "            else:\n",
    "                for src_text in text_data['src_text']:\n",
    "                    token_data.append(tokenizer(src_text, max_length=max_length, padding='max_length', truncation=True))\n",
    "\n",
    "        return token_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b008e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split,  DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from typing import Optional\n",
    "from utils import load_raw_data, predict\n",
    "#from dataset import load_dataset\n",
    "#import LanguageDataset\n",
    "\n",
    "class LanugageDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    DataModule used for semantic segmentation in geometric generalization project\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,data, eval, batch_size, numOfWorker = 8):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.model_name = model_name\n",
    "        self.numOfWorker = numOfWorker\n",
    "      \n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Empty prepare_data method left in intentionally. \n",
    "        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html#prepare-data\n",
    "        \"\"\"\n",
    "        pass\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Method to setup your datasets, here you can use whatever dataset class you have defined in Pytorch and prepare the data in order to pass it to the loaders later\n",
    "        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html#setup\n",
    "        \"\"\"\n",
    "      \n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        # the stage is used in the Pytorch Lightning trainer method, which you can call as fit (training, evaluation) or test, also you can use it for predict, not implemented here\n",
    "        \n",
    "        if stage == \"fit\" or stage is None:\n",
    "            train_set_full =  LanguageDataset(self.data)\n",
    "            train_set_size = int(len(train_set_full) * 0.9)\n",
    "            valid_set_size = len(train_set_full) - train_set_size\n",
    "            self.train, self.validate = random_split(train_set_full, [train_set_size, valid_set_size])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test = LanguageDataset()\n",
    "            \n",
    "    # define your dataloaders\n",
    "    # again, here defined for train, validate and test, not for predict as the project is not there yet. \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size, shuffle=True, num_workers=self.numOfWorker)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.validate, batch_size=self.batch_size, num_workers=self.numOfWorker)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.validate, batch_size=self.batch_size, num_workers=self.numOfWorker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6071f77",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20560\\1945110714.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m      'nah_Latn', 'quy_Latn', 'tar_Latn', 'shp_Latn', 'hch_Latn']\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_raw_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_src_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_trg_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[0meval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_raw_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_src_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_trg_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\research\\ameircasnlp2023\\Lightening\\utils.py\u001b[0m in \u001b[0;36mload_raw_data\u001b[1;34m(src_filepath, lang_code, model_name, trg_filepath, max_length)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0msrc_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg_text\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'src_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                 token_data.append(tokenizer(src_text, text_target=trg_text, \n\u001b[0m\u001b[0;32m     44\u001b[0m                                             max_length=max_length, padding='max_length', truncation=True))\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2524\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2526\u001b[1;33m             \u001b[0mtarget_encodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_pair_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2527\u001b[0m         \u001b[1;31m# Leave back tokenizer in input mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2528\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2627\u001b[0m             )\n\u001b[0;32m   2628\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2629\u001b[1;33m             return self.encode_plus(\n\u001b[0m\u001b[0;32m   2630\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2631\u001b[0m                 \u001b[0mtext_pair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2700\u001b[0m         )\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2702\u001b[1;33m         return self._encode_plus(\n\u001b[0m\u001b[0;32m   2703\u001b[0m             \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2704\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[0mbatched_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m         batched_output = self._batch_encode_plus(\n\u001b[0m\u001b[0;32m    503\u001b[0m             \u001b[0mbatched_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m             \u001b[0mis_split_into_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    427\u001b[0m         )\n\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[0;32m    430\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from lightning_transformers.task.nlp.translation import (\n",
    "    TranslationTransformer,\n",
    "    WMT16TranslationDataModule,\n",
    ")\n",
    "main_folder =  '../processed_data/'\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "train_src_filepath = [main_folder+'ashaninka/dedup_filtered.cni',\n",
    "              main_folder+'aymara/dedup_filtered.aym',\n",
    "              main_folder+'bribri/dedup_filtered.bzd',\n",
    "              main_folder+'guarani/dedup_filtered.gn',\n",
    "              main_folder+'hñähñu/dedup_filtered.oto',\n",
    "              main_folder+'nahuatl/dedup_filtered.nah',\n",
    "              main_folder+'quechua/dedup_filtered.quy',\n",
    "              main_folder+'raramuri/dedup_filtered.tar',\n",
    "              main_folder+'shipibo_konibo/dedup_filtered.shp',\n",
    "              main_folder+'wixarika/dedup_filtered.hch']\n",
    "\n",
    "train_trg_filepath = [main_folder+'ashaninka/dedup_filtered.es',\n",
    "                      main_folder+'aymara/dedup_filtered.es',\n",
    "                      main_folder+'bribri/dedup_filtered.es',\n",
    "                      main_folder+'guarani/dedup_filtered.es',\n",
    "                      main_folder+'hñähñu/dedup_filtered.es',\n",
    "                      main_folder+'nahuatl/dedup_filtered.es',\n",
    "                      main_folder+'quechua/dedup_filtered.es',\n",
    "                      main_folder+'raramuri/dedup_filtered.es',\n",
    "                      main_folder+'shipibo_konibo/dedup_filtered.es',\n",
    "                      main_folder+'wixarika/dedup_filtered.es']\n",
    "\n",
    "eval_src_filepath = [main_folder+'ashaninka/dev.cni',\n",
    "                     main_folder+'aymara/dev.aym',\n",
    "                     main_folder+'bribri/dev.bzd',\n",
    "                     main_folder+'guarani/dev.gn',\n",
    "                     main_folder+'hñähñu/dev.oto',\n",
    "                     main_folder+'nahuatl/dev.nah',\n",
    "                     main_folder+'quechua/dev.quy',\n",
    "                     main_folder+'raramuri/dev.tar',\n",
    "                     main_folder+'shipibo_konibo/dev.shp',\n",
    "                     main_folder+'wixarika/dev.hch']\n",
    "\n",
    "eval_trg_filepath = [main_folder+'ashaninka/dev.es',\n",
    "                     main_folder+'aymara/dev.es',\n",
    "                     main_folder+'bribri/dev.es',\n",
    "                     main_folder+'guarani/dev.es',\n",
    "                     main_folder+'hñähñu/dev.es',\n",
    "                     main_folder+'nahuatl/dev.es',\n",
    "                     main_folder+'quechua/dev.es',\n",
    "                     main_folder+'raramuri/dev.es',\n",
    "                     main_folder+'shipibo_konibo/dev.es',\n",
    "                     main_folder+'wixarika/dev.es']\n",
    "lang_code = ['cni_Latn', 'aym_Latn', 'bzd_Latn', 'gn_Latn', 'oto_Latn', \n",
    "     'nah_Latn', 'quy_Latn', 'tar_Latn', 'shp_Latn', 'hch_Latn']\n",
    "\n",
    "data = load_raw_data(train_src_filepath, lang_code, model_name, train_trg_filepath, max_length=128)\n",
    "eval = load_raw_data(eval_src_filepath, lang_code, model_name, eval_trg_filepath, max_length=128)\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "dm = LanugageDataModule(\n",
    "    # WMT translation datasets: ['cs-en', 'de-en', 'fi-en', 'ro-en', 'ru-en', 'tr-en']\n",
    "    data = data,\n",
    "    eval = eval,\n",
    "    batch_size = 32,\n",
    "\n",
    ")\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=\"auto\", max_epochs=1)\n",
    "\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71b8dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
